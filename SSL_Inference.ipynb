{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y822XzLTZbYK",
        "outputId": "8e141d43-66d4-449c-9816-32089fa38738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "# import packages here\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import albumentations\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VREoLi-UZjbY",
        "outputId": "8447b6d1-4e66-4bef-8711-aec793b833cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/gdrive/My Drive/data/'"
      ],
      "metadata": {
        "id": "B2Jj-AThZn-e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "m8RpJNX4dRqA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformations for testing\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Assuming you have a test/validation dataset in the same format as your training data\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'test/'), transform=test_transform)\n",
        "\n",
        "# Create DataLoader for test/validation\n",
        "testloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "id": "WyzDflwxb8zH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MoCo model\n",
        "class MoCo(nn.Module):\n",
        "    def __init__(self, base_encoder, dim=128, K=131072, m=0.999, T=0.07):\n",
        "        super(MoCo, self).__init__()\n",
        "\n",
        "        self.K = K  # Size of the queue\n",
        "        self.m = m  # Momentum coefficient\n",
        "        self.T = T  # Temperature for contrastive loss\n",
        "\n",
        "        # Create the query encoder\n",
        "        self.encoder_q = base_encoder(num_classes=dim)\n",
        "        self.encoder_k = base_encoder(num_classes=dim)\n",
        "\n",
        "        # Initialize the momentum encoder with the same weights\n",
        "        self._init_momentum_encoder()\n",
        "\n",
        "        # Create the queue\n",
        "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
        "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "    def _init_momentum_encoder(self):\n",
        "        # Initialize momentum encoder parameters to match the query encoder\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)  # Copy weights\n",
        "            param_k.requires_grad = False  # Do not update momentum encoder with gradients\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update_key_encoder(self):\n",
        "        # Update the momentum encoder with momentum\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequeue_and_enqueue(self, keys):\n",
        "        # Update the queue\n",
        "        batch_size = keys.shape[0]  # Use the actual batch size\n",
        "\n",
        "        ptr = int(self.queue_ptr)\n",
        "        assert self.K % batch_size == 0 or batch_size <= self.K  # Ensure the batch size fits in the queue\n",
        "\n",
        "        # Replace the keys at ptr (dequeue and enqueue), handling small batches correctly\n",
        "        end_ptr = min(ptr + batch_size, self.K)\n",
        "        effective_batch_size = end_ptr - ptr\n",
        "\n",
        "        self.queue[:, ptr:end_ptr] = keys[:effective_batch_size].T  # Dequeue and enqueue the keys\n",
        "        ptr = (ptr + effective_batch_size) % self.K  # Move the pointer\n",
        "\n",
        "        self.queue_ptr[0] = ptr\n",
        "\n",
        "    def forward(self, im_q, im_k):\n",
        "        # Compute query features\n",
        "        q = self.encoder_q(im_q)  # Query image\n",
        "        q = F.normalize(q, dim=1)\n",
        "\n",
        "        # Compute key features using momentum encoder\n",
        "        with torch.no_grad():\n",
        "            self._momentum_update_key_encoder()  # Update the key encoder\n",
        "            k = self.encoder_k(im_k)  # Key image\n",
        "            k = F.normalize(k, dim=1)\n",
        "\n",
        "        # Compute logits and contrastive loss\n",
        "        # Positive logits: dot product between query and key\n",
        "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
        "\n",
        "        # Negative logits: dot product between query and all negatives in the queue\n",
        "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
        "\n",
        "        # Logits: [N, 1 + K]\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        logits /= self.T\n",
        "\n",
        "        # Labels: positive key is the first in the logit list\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(im_q.device)\n",
        "\n",
        "        # Dequeue and enqueue the current mini-batch of keys\n",
        "        self._dequeue_and_enqueue(k)\n",
        "\n",
        "        return logits, labels\n"
      ],
      "metadata": {
        "id": "fptEUxFCZqN4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Skip connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity  # Add skip connection (residual)\n",
        "        return F.relu(out)\n",
        "\n",
        "# Define the ResNetCNN model with Dropout (used as the base encoder)\n",
        "class ResNetCNN(nn.Module):\n",
        "    def __init__(self, num_classes=16):  # Adjust num_classes based on your dataset\n",
        "        super(ResNetCNN, self).__init__()\n",
        "        self.layer1 = ResidualBlock(3, 64)  # Start with 3 channels (RGB input)\n",
        "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
        "        self.layer4 = ResidualBlock(256, 512, stride=2)\n",
        "        self.fc = nn.Linear(512 * 16 * 16, 128)  # MoCo latent dimension\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, num_classes),  # Add classifier layer\n",
        "            nn.Dropout(0.5)  # 50% dropout to avoid overfitting\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc(x)  # Pass through MoCo latent fc layer\n",
        "        return x\n",
        "\n",
        "    def forward_for_classification(self, x):\n",
        "        x = self.forward(x)  # Get the output from the encoder\n",
        "        x = self.classifier(x)  # Pass through the classification layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "TaSlyIvWaePa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moco_model = MoCo(ResNetCNN, dim=128, K=131072, m=0.999, T=0.07)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "moco_model.load_state_dict(torch.load('/content/gdrive/My Drive/data/moco_model.pth'))\n",
        "moco_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy1P8TEbcZhp",
        "outputId": "c64c7bfb-199c-45cb-cb96-86ee6b7a57fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d851bf5e7421>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  moco_model.load_state_dict(torch.load('/content/gdrive/My Drive/data/moco_model.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MoCo(\n",
              "  (encoder_q): ResNetCNN(\n",
              "    (layer1): ResidualBlock(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): ResidualBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): ResidualBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (fc): Linear(in_features=131072, out_features=128, bias=True)\n",
              "    (classifier): Sequential(\n",
              "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (1): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (encoder_k): ResNetCNN(\n",
              "    (layer1): ResidualBlock(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): ResidualBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): ResidualBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (fc): Linear(in_features=131072, out_features=128, bias=True)\n",
              "    (classifier): Sequential(\n",
              "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (1): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, testloader, criterion):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass through the query encoder and classification layer\n",
        "            outputs = model.encoder_q.forward_for_classification(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(testloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "evaluate_model(moco_model, testloader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5P_120ncOUh",
        "outputId": "49415fd6-d048-4838-ebfb-e72275c6db39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.2948, Test Accuracy: 67.75%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2947775295802526, 67.75)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}